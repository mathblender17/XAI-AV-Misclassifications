{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35db8415",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-28T21:34:53.819310Z",
     "iopub.status.busy": "2025-04-28T21:34:53.818974Z",
     "iopub.status.idle": "2025-04-28T21:36:02.710463Z",
     "shell.execute_reply": "2025-04-28T21:36:02.709462Z"
    },
    "papermill": {
     "duration": 68.903312,
     "end_time": "2025-04-28T21:36:02.712454",
     "exception": false,
     "start_time": "2025-04-28T21:34:53.809142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6108e",
   "metadata": {
    "papermill": {
     "duration": 0.033087,
     "end_time": "2025-04-28T21:36:02.777214",
     "exception": false,
     "start_time": "2025-04-28T21:36:02.744127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbf120f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T21:36:02.847727Z",
     "iopub.status.busy": "2025-04-28T21:36:02.847411Z",
     "iopub.status.idle": "2025-04-28T21:36:12.196425Z",
     "shell.execute_reply": "2025-04-28T21:36:12.195604Z"
    },
    "papermill": {
     "duration": 9.38734,
     "end_time": "2025-04-28T21:36:12.197909",
     "exception": false,
     "start_time": "2025-04-28T21:36:02.810569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# !pip install --force-reinstall numpy scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99fbe5",
   "metadata": {
    "papermill": {
     "duration": 0.022432,
     "end_time": "2025-04-28T21:36:12.243922",
     "exception": false,
     "start_time": "2025-04-28T21:36:12.221490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image prediction / classifer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1cbe58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T21:36:12.289712Z",
     "iopub.status.busy": "2025-04-28T21:36:12.289308Z",
     "iopub.status.idle": "2025-04-28T21:36:13.630713Z",
     "shell.execute_reply": "2025-04-28T21:36:13.629666Z"
    },
    "papermill": {
     "duration": 1.365793,
     "end_time": "2025-04-28T21:36:13.631968",
     "exception": true,
     "start_time": "2025-04-28T21:36:12.266175",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "/kaggle/working/cropped_objects/4_Truck.png does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/2850186982.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/cropped_objects/4_Truck.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# Display predictions inline (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"set_prompts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for SAM-type models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     def track(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# merge list of Result into one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for thread-safe inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;31m# Setup source every time predict is called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;31m# Check if save_dir/ label file exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36msetup_source\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         )\n\u001b[0;32m--> 259\u001b[0;31m         self.dataset = load_inference_source(\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/data/build.py\u001b[0m in \u001b[0;36mload_inference_source\u001b[0;34m(source, batch, vid_stride, buffer)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadPilAndNumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadImagesAndVideos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvid_stride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Attach source types to the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/data/loaders.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, batch, vid_stride)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# files (relative to *.txt file parent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{p} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# Define files as images or videos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /kaggle/working/cropped_objects/4_Truck.png does not exist"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load the YOLOv11 model\n",
    "#model = YOLO(\"/kaggle/input/yolo11_trained/pytorch/default/1/yolo11m.pt\")\n",
    "model = YOLO(\"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\")\n",
    "# Predict on a sample image\n",
    "#image_path =   \"/kaggle/input/kitti-dataset/data_object_image_2/testing/image_2/000007.png\" \n",
    "# image_path = \"/kaggle/input/kitti-dataset/data_object_image_3/testing/image_3/000004.png\"\n",
    "image_path = \"/kaggle/working/cropped_objects/4_Truck.png\"\n",
    "\n",
    "results = model.predict(source=image_path, conf=0.25, save=False)\n",
    "# Display predictions inline (optional)\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1b3d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Grad CAM current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bfaac9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T19:59:31.383294Z",
     "iopub.status.busy": "2025-04-28T19:59:31.382675Z",
     "iopub.status.idle": "2025-04-28T19:59:32.278742Z",
     "shell.execute_reply": "2025-04-28T19:59:32.278019Z",
     "shell.execute_reply.started": "2025-04-28T19:59:31.383272Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i = 7\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load YOLOv11 model\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Load and preprocess image\n",
    "#image_path = \"/kaggle/input/kitti-dataset/data_object_image_2/testing/image_2/000007.png\"\n",
    "image_path = \"/kaggle/input/ig-test-on-1st-car-in-oooo7/save as IG test.png\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(( 320,736)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(original_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad_()\n",
    "\n",
    "# Get class names and find 'Car' index\n",
    "class_names = model.names\n",
    "car_class_index = [k for k, v in class_names.items() if v.lower() == \"car\"]\n",
    "#tram_class_index = [k for k, v in class_names.items() if v.lower() == \"tram\"]\n",
    "if not car_class_index:\n",
    "    raise ValueError(\"Car class not found.\")\n",
    "car_class_index = car_class_index[0]\n",
    "#tram_class_index = tram_class_index[0]\n",
    "\n",
    "# Hook Grad-CAM\n",
    "activations = None\n",
    "gradients = None\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    global activations\n",
    "    activations = output\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    global gradients\n",
    "    gradients = grad_output[0]\n",
    "\n",
    "# Register hooks\n",
    "target_layer = model.model.model[16]  # Should be a deep conv layer\n",
    "target_layer.register_forward_hook(forward_hook)\n",
    "target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "# Forward pass through raw model\n",
    "output = model.model(input_tensor)[0]  # shape: (1, N, 5 + num_classes)\n",
    "output = output[0]  # Remove batch dim\n",
    "\n",
    "# Compute car class scores\n",
    "obj_conf = output[:, 4]\n",
    "class_probs = output[:, 5:]\n",
    "car_scores = obj_conf * class_probs[:, car_class_index]\n",
    "\n",
    "# Select top car\n",
    "if car_scores.max() == 0:\n",
    "    raise ValueError(\"No car detected in raw output.\")\n",
    "    \n",
    "top_idx = torch.argmax(car_scores)\n",
    "score = car_scores[top_idx]\n",
    "# Get top-k car detections\n",
    "# top_k = 5  # You can tune this number\n",
    "# topk_scores, topk_indices = torch.topk(car_scores, k=top_k)\n",
    "\n",
    "# # Filter zero-score predictions\n",
    "# non_zero = topk_scores > 0\n",
    "# if non_zero.sum() == 0:\n",
    "#     raise ValueError(\"No high-scoring car predictions.\")\n",
    "# filtered_scores = topk_scores[non_zero]\n",
    "\n",
    "# # Take mean score for Grad-CAM\n",
    "# score = filtered_scores.mean()\n",
    "\n",
    "\n",
    "\n",
    "# Backward pass for Grad-CAM\n",
    "model.model.zero_grad()\n",
    "score.backward(retain_graph=True)\n",
    "\n",
    "# Grad-CAM computation\n",
    "weights = gradients.mean(dim=(2, 3), keepdim=True)\n",
    "cam = (weights * activations).sum(dim=1, keepdim=True)\n",
    "cam = F.relu(cam)\n",
    "\n",
    "# Normalize CAM using Clipping Percentiles\n",
    "cam = cam.squeeze().detach().cpu().numpy()\n",
    "cam = cv2.resize(cam, (640, 640))\n",
    "\n",
    "# Compute 1st and 99th percentiles\n",
    "p1, p99 = np.percentile(cam, 1), np.percentile(cam, 99)\n",
    "\n",
    "# Clip and rescale to [0, 1]\n",
    "cam = np.clip(cam, p1, p99)\n",
    "cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-6)\n",
    "\n",
    "# Create heatmap overlay\n",
    "original_np = np.array(original_image.resize((640, 640))) / 255.0\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "heatmap = np.float32(heatmap) / 255\n",
    "overlay = heatmap + original_np\n",
    "overlay = overlay / np.max(overlay)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Grad-CAM Heatmap\")\n",
    "plt.imshow(cam, cmap=\"jet\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Overlay\")\n",
    "plt.imshow(overlay)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show detection result\n",
    "results[0].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4aac45",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### gradCAM with prevuilt libaries - the prebuilt library is not supported in kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145421b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T15:45:31.329387Z",
     "iopub.status.busy": "2025-04-28T15:45:31.328676Z",
     "iopub.status.idle": "2025-04-28T15:45:32.901918Z",
     "shell.execute_reply": "2025-04-28T15:45:32.901155Z",
     "shell.execute_reply.started": "2025-04-28T15:45:31.329365Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q pytorch-grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b3b14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install if not done\n",
    "# !pip install -q pytorch-grad-cam\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# Load YOLOv11 model\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Load and preprocess image\n",
    "image_path = \"/kaggle/input/red-car-on-road/road-landscape-red-car-beautiful-260nw-2377086843.png\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(original_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Find 'car' class index\n",
    "class_names = model.names\n",
    "car_class_index = [k for k, v in class_names.items() if v.lower() == \"car\"]\n",
    "if not car_class_index:\n",
    "    raise ValueError(\"Car class not found.\")\n",
    "car_class_index = car_class_index[0]\n",
    "\n",
    "# Forward pass through raw model\n",
    "output = model.model(input_tensor)[0]  # shape: (1, N, 5 + num_classes)\n",
    "output = output[0]  # Remove batch dim\n",
    "\n",
    "# Compute car class scores\n",
    "obj_conf = output[:, 4]\n",
    "class_probs = output[:, 5:]\n",
    "car_scores = obj_conf * class_probs[:, car_class_index]\n",
    "\n",
    "# Select top car\n",
    "if car_scores.max() == 0:\n",
    "    raise ValueError(\"No car detected in raw output.\")\n",
    "top_idx = torch.argmax(car_scores)\n",
    "\n",
    "# -------------------------------\n",
    "# Grad-CAM Part (library version)\n",
    "# -------------------------------\n",
    "\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# You must define a custom target\n",
    "class DetectionTarget:\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        \"\"\"\n",
    "        model_output shape: [num_preds, 5 + num_classes]\n",
    "        We are focusing on objectness * class prob of 'car' at specific index\n",
    "        \"\"\"\n",
    "        obj_conf = model_output[:, 4]\n",
    "        class_probs = model_output[:, 5:]\n",
    "        car_score = obj_conf * class_probs[:, car_class_index]\n",
    "        return car_score[self.index]\n",
    "\n",
    "# Select a good layer\n",
    "target_layer = model.model.model[16]  # Same deep conv layer\n",
    "\n",
    "cam = GradCAM(model=model.model, target_layers=[target_layer], use_cuda=torch.cuda.is_available())\n",
    "\n",
    "targets = [DetectionTarget(index=top_idx)]\n",
    "\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]  # [0] since batch size = 1\n",
    "\n",
    "# Prepare visualization\n",
    "original_np = np.array(original_image.resize((640, 640))) / 255.0\n",
    "visualization = show_cam_on_image(original_np, grayscale_cam, use_rgb=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Grad-CAM Heatmap\")\n",
    "plt.imshow(grayscale_cam, cmap=\"jet\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Overlay\")\n",
    "plt.imshow(visualization)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optionally show detection result\n",
    "results = model.predict(image_path)\n",
    "results[0].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235e93c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Grad CAm error solving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315167e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T15:10:49.506576Z",
     "iopub.status.busy": "2025-04-28T15:10:49.505998Z",
     "iopub.status.idle": "2025-04-28T15:10:50.129922Z",
     "shell.execute_reply": "2025-04-28T15:10:50.129032Z",
     "shell.execute_reply.started": "2025-04-28T15:10:49.506547Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load YOLOv11 model\n",
    "#model_path = \"/kaggle/input/trained_best-model/pytorch/default/1/best.pt\"\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Load and preprocess image\n",
    "image_path = \"/kaggle/input/kitti-dataset/data_object_image_2/testing/image_2/000007.png\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((384, 1248)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(original_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad_()\n",
    "\n",
    "# Get class names\n",
    "class_names = model.names  # dict {id: name}\n",
    "\n",
    "# Hook Grad-CAM\n",
    "activations = None\n",
    "gradients = None\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    global activations\n",
    "    activations = output\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    global gradients\n",
    "    gradients = grad_output[0]\n",
    "\n",
    "# Register hooks\n",
    "target_layer = model.model.model[16]  # Should be a deep conv layer\n",
    "target_layer.register_forward_hook(forward_hook)\n",
    "target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "# Forward pass through YOLO model\n",
    "#preds = model.predict(original_image, imgsz=(384, 1248), conf=0.3, device=device)[0]\n",
    "preds = model.predict(input_tensor, imgsz=(384, 1248), conf=0.3, device=device)[0]\n",
    "\n",
    "pred_classes = preds.boxes.cls.int().tolist()\n",
    "\n",
    "# Get unique detected classes\n",
    "detected_class_indices = list(set(pred_classes))\n",
    "print(f\"âœ… Detected classes: {[class_names[idx] for idx in detected_class_indices]}\")\n",
    "\n",
    "# Forward pass through raw model for Grad-CAM\n",
    "output = model.model(input_tensor)[0][0]  # Remove batch dim\n",
    "obj_conf = output[:, 4]\n",
    "class_probs = output[:, 5:]\n",
    "\n",
    "# Prepare original image as numpy\n",
    "#original_np = np.array(original_image.resize()) / 255.0\n",
    "original_np = np.array(original_image.resize((1248, 384))) / 255.0\n",
    "\n",
    "cams_per_class = {}\n",
    "\n",
    "# Grad-CAM for each detected class\n",
    "for class_idx in detected_class_indices:\n",
    "    class_name = class_names[class_idx]\n",
    "    print(f\"ğŸ”µ Creating Grad-CAM for {class_name}\")\n",
    "    \n",
    "    class_scores = obj_conf * class_probs[:, class_idx]\n",
    "    if class_scores.max() == 0:\n",
    "        print(f\"âš ï¸ No {class_name} detected in raw output. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    top_idx = torch.argmax(class_scores)\n",
    "    score = class_scores[top_idx]\n",
    "    \n",
    "    model.model.zero_grad()\n",
    "    score.backward(retain_graph=True)\n",
    "    \n",
    "    weights = gradients.mean(dim=(2, 3), keepdim=True)\n",
    "    cam = (weights * activations).sum(dim=1, keepdim=True)\n",
    "    cam = F.relu(cam).squeeze().detach().cpu().numpy()\n",
    "    cam = cv2.resize(cam, (1248, 384))\n",
    "    \n",
    "    p1, p99 = np.percentile(cam, 1), np.percentile(cam, 99)\n",
    "    cam = np.clip(cam, p1, p99)\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-6)\n",
    "    \n",
    "    cams_per_class[class_name] = cam\n",
    "\n",
    "# ===================== ğŸ”¥ Updated Visualization ğŸ”¥ =====================\n",
    "\n",
    "num_classes_detected = len(cams_per_class)\n",
    "total_images = num_classes_detected + 1  # +1 for the original image\n",
    "\n",
    "#plt.figure(figsize=(5 * total_images, 5))  # Wide figure for 1 row\n",
    "fig_width = min(5 * total_images, 30)  # max width 30 inches\n",
    "plt.figure(figsize=(fig_width, 5))\n",
    "\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, total_images, 1)\n",
    "plt.title(\"Original Image\", fontsize=14, color='black')\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Individual CAMs\n",
    "for idx, (class_name, cam) in enumerate(cams_per_class.items(), start=2):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    overlay = heatmap + original_np\n",
    "    overlay = overlay / np.max(overlay)\n",
    "    \n",
    "    plt.subplot(1, total_images, idx)\n",
    "    plt.title(f\"{class_name} - Grad-CAM\", fontsize=14, color='black')\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82290666",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fed427b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## IG \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4045e",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-04-28T18:47:04.136549Z",
     "iopub.status.busy": "2025-04-28T18:47:04.136133Z",
     "iopub.status.idle": "2025-04-28T18:47:07.589764Z",
     "shell.execute_reply": "2025-04-28T18:47:07.589044Z",
     "shell.execute_reply.started": "2025-04-28T18:47:04.136530Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install captum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4354c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T15:11:02.226419Z",
     "iopub.status.busy": "2025-04-28T15:11:02.225854Z",
     "iopub.status.idle": "2025-04-28T15:11:02.427178Z",
     "shell.execute_reply": "2025-04-28T15:11:02.426265Z",
     "shell.execute_reply.started": "2025-04-28T15:11:02.226393Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# --- Define Baseline ---\n",
    "# Same size as input, black image\n",
    "baseline = torch.zeros_like(input_tensor).to(device)\n",
    "\n",
    "# --- Define prediction function ---\n",
    "def predict(inputs):\n",
    "    outputs = model.model(inputs)[0]  # shape: (1, N, 5+num_classes)\n",
    "    outputs = outputs[0]  # Remove batch dim\n",
    "    \n",
    "    obj_conf = outputs[:, 4]\n",
    "    class_probs = outputs[:, 5:]\n",
    "    car_scores = obj_conf * class_probs[:, car_class_index]\n",
    "    \n",
    "    top_idx = torch.argmax(car_scores)\n",
    "    score = car_scores[top_idx]\n",
    "    \n",
    "    return score\n",
    "\n",
    "# --- Create Integrated Gradients object ---\n",
    "ig = IntegratedGradients(predict)\n",
    "\n",
    "# --- Compute attributions ---\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs=input_tensor,\n",
    "    baselines=baseline,\n",
    "    target=None,   # because we already handle scoring inside predict\n",
    "    return_convergence_delta=True,\n",
    "    n_steps=5,    # More steps = smoother but slower\n",
    ")\n",
    "\n",
    "# --- Process attributions ---\n",
    "# Take absolute value and sum across channels\n",
    "attr = attributions.squeeze().detach().cpu().numpy()\n",
    "attr = np.sum(np.abs(attr), axis=0)\n",
    "\n",
    "# Normalize for visualization\n",
    "attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-6)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Integrated Gradients\")\n",
    "plt.imshow(attr, cmap=\"viridis\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Overlay\")\n",
    "heatmap_ig = cv2.applyColorMap(np.uint8(255 * attr), cv2.COLORMAP_VIRIDIS)\n",
    "heatmap_ig = np.float32(heatmap_ig) / 255\n",
    "overlay_ig = heatmap_ig + original_np\n",
    "overlay_ig = overlay_ig / np.max(overlay_ig)\n",
    "plt.imshow(overlay_ig)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee75091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T06:40:59.454855Z",
     "iopub.status.busy": "2025-04-27T06:40:59.454075Z",
     "iopub.status.idle": "2025-04-27T06:41:00.496119Z",
     "shell.execute_reply": "2025-04-27T06:41:00.495170Z",
     "shell.execute_reply.started": "2025-04-27T06:40:59.454830Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# --- Optional: Speed up conv layers ---\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- Load Model ---\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# --- Load and preprocess image ---\n",
    "image_path = \"/kaggle/input/kitti-dataset/data_object_image_2/testing/image_2/000007.png\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Resize to 384 x 640\n",
    "resize_size = (640, 384)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(resize_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(original_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad_()\n",
    "\n",
    "# --- Baseline (black image) ---\n",
    "baseline = torch.zeros_like(input_tensor).to(device)\n",
    "\n",
    "# --- Class Names ---\n",
    "class_names = model.names\n",
    "car_class_index = [k for k, v in class_names.items() if v.lower() == \"car\"]\n",
    "if not car_class_index:\n",
    "    raise ValueError(\"Car class not found.\")\n",
    "car_class_index = car_class_index[0]\n",
    "\n",
    "# --- Define prediction function ---\n",
    "# def predict(inputs):\n",
    "#     outputs = model.model(inputs)[0]  # shape: (1, N, 5+num_classes)\n",
    "#     outputs = outputs[0]  # remove batch dim\n",
    "#     obj_conf = outputs[:, 4]\n",
    "#     class_probs = outputs[:, 5:]\n",
    "#     car_scores = obj_conf * class_probs[:, car_class_index]\n",
    "#     top_idx = torch.argmax(car_scores)\n",
    "#     score = car_scores[top_idx]\n",
    "#     return score\n",
    "def predict(inputs):\n",
    "    outputs = model.model(inputs)[0]  # shape: (1, N, 5+num_classes)\n",
    "    outputs = outputs[0]  # remove batch dim\n",
    "    obj_conf = outputs[:, 4]\n",
    "    class_probs = outputs[:, 5:]\n",
    "    car_scores = obj_conf * class_probs[:, car_class_index]\n",
    "    top_idx = torch.argmax(car_scores)\n",
    "    score = car_scores[top_idx]\n",
    "    score = score.unsqueeze(0)  # <--- fix for 0D tensors\n",
    "    return score\n",
    "\n",
    "\n",
    "# --- Integrated Gradients object ---\n",
    "ig = IntegratedGradients(predict)\n",
    "\n",
    "# --- Compute attributions ---\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs=input_tensor,\n",
    "    baselines=baseline,\n",
    "    target=None,\n",
    "    return_convergence_delta=True,\n",
    "    n_steps=20,               # fewer steps\n",
    "    internal_batch_size=4     # mini-batches\n",
    ")\n",
    "\n",
    "# --- Process attributions ---\n",
    "attr = attributions.squeeze().detach().cpu().numpy()\n",
    "attr = np.sum(np.abs(attr), axis=0)  # sum over channels\n",
    "attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-6)  # normalize\n",
    "\n",
    "# --- Prepare original resized image for overlay ---\n",
    "original_np = np.array(original_image.resize(resize_size)) / 255.0\n",
    "\n",
    "# # Before combining\n",
    "# original_np = cv2.resize(original_np, (heatmap_ig.shape[1], heatmap_ig.shape[0]))\n",
    "\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Integrated Gradients\")\n",
    "plt.imshow(attr, cmap=\"viridis\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Overlay\")\n",
    "heatmap_ig = cv2.applyColorMap(np.uint8(255 * attr), cv2.COLORMAP_VIRIDIS)\n",
    "heatmap_ig = np.float32(heatmap_ig) / 255\n",
    "overlay_ig = heatmap_ig + original_np\n",
    "overlay_ig = overlay_ig / np.max(overlay_ig)\n",
    "plt.imshow(overlay_ig)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961426b8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# IG Implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22687e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# Load the image (in color)\n",
    "image = cv2.imread(\"/kaggle/input/kitti-dataset/data_object_image_2/testing/image_2/000007.png\")  # This loads as BGR by default\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Save or display the grayscale image (optional)\n",
    "cv2.imwrite(\"path_to_save_gray_image.jpg\", gray_image)\n",
    "\n",
    "# To display (for testing):\n",
    "cv2.imshow(\"Grayscale Image\", gray_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930531c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T20:03:52.711402Z",
     "iopub.status.busy": "2025-04-28T20:03:52.711080Z",
     "iopub.status.idle": "2025-04-28T20:04:02.490278Z",
     "shell.execute_reply": "2025-04-28T20:04:02.489093Z",
     "shell.execute_reply.started": "2025-04-28T20:03:52.711379Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from captum.attr import IntegratedGradients\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# --- Speed up ---\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- Load Model ---\n",
    "model_path = \"/kaggle/input/yolov11m/pytorch/default/1/yolo11m.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# --- Load and preprocess image ---\n",
    "image_path = \"/kaggle/input/ig-test-on-1st-car-in-oooo7/save as IG test.png\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "resize_size = (640,640)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(resize_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(original_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad_()\n",
    "\n",
    "# --- Blurred baseline ---\n",
    "def blur_baseline(image_tensor, kernel_size=51):\n",
    "    blurred = TF.gaussian_blur(image_tensor.cpu().squeeze(0), kernel_size=kernel_size)\n",
    "    blurred = blurred.unsqueeze(0).to(image_tensor.device)\n",
    "    return blurred\n",
    "\n",
    "baseline = blur_baseline(input_tensor, kernel_size=51)\n",
    "\n",
    "# --- Class Names ---\n",
    "class_names = model.names\n",
    "car_class_index = [k for k, v in class_names.items() if v.lower() == \"car\"]\n",
    "if not car_class_index:\n",
    "    raise ValueError(\"Car class not found.\")\n",
    "car_class_index = car_class_index[0]\n",
    "\n",
    "# --- Define prediction function ---\n",
    "def predict(inputs):\n",
    "    outputs = model.model(inputs)[0]  # shape: (1, N, 5+num_classes)\n",
    "    outputs = outputs[0]\n",
    "    obj_conf = outputs[:, 4]\n",
    "    class_probs = outputs[:, 5:]\n",
    "    car_scores = obj_conf * class_probs[:, car_class_index]\n",
    "    top_idx = torch.argmax(car_scores)\n",
    "    score = car_scores[top_idx]\n",
    "    return score.unsqueeze(0)\n",
    "\n",
    "# --- Integrated Gradients ---\n",
    "ig = IntegratedGradients(predict)\n",
    "\n",
    "# --- Compute attributions ---\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs=input_tensor,\n",
    "    baselines=baseline,\n",
    "    target=None,\n",
    "    return_convergence_delta=True,\n",
    "    n_steps=300,               # more steps = better quality\n",
    "    internal_batch_size=6\n",
    ")\n",
    "\n",
    "# --- Process attributions ---\n",
    "attr = attributions.squeeze().detach().cpu().numpy()\n",
    "attr = np.sum(np.abs(attr), axis=0)  # sum over channels\n",
    "attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-6)  # normalize\n",
    "\n",
    "# --- Prepare image for visualization ---\n",
    "original_np = np.array(original_image.resize(resize_size)) / 255.0\n",
    "\n",
    "# --- Draw bounding boxes ---\n",
    "results = model.predict(original_image, imgsz=resize_size, conf=0.3)\n",
    "boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    cv2.rectangle(original_np, (x1, y1), (x2, y2), color=(1,0,0), thickness=2)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# Plot Original Image with Bounding Boxes\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Original + Detections\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot Integrated Gradients Attribution Map\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Integrated Gradients\")\n",
    "plt.imshow(attr, cmap=\"\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot Overlay (heatmap + original)\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Overlay\")\n",
    "heatmap_ig = cv2.applyColorMap(np.uint8(255 * attr), cv2.COLORMAP_JET)\n",
    "heatmap_ig = np.float32(heatmap_ig) / 255\n",
    "overlay_ig = heatmap_ig + original_np\n",
    "overlay_ig = overlay_ig / np.max(overlay_ig)\n",
    "plt.imshow(overlay_ig)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e6be7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T20:01:10.560039Z",
     "iopub.status.busy": "2025-04-28T20:01:10.559795Z",
     "iopub.status.idle": "2025-04-28T20:01:11.350377Z",
     "shell.execute_reply": "2025-04-28T20:01:11.349657Z",
     "shell.execute_reply.started": "2025-04-28T20:01:10.560022Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Run detection on the resized input image ---\n",
    "detections = model.predict(original_image.resize(resize_size), conf=0.25)[0]  # confidence threshold 25%\n",
    "\n",
    "# --- Extract 'car' detections ---\n",
    "car_boxes = []\n",
    "for box in detections.boxes:\n",
    "    if int(box.cls.item()) == car_class_index:\n",
    "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "        car_boxes.append((int(x1), int(y1), int(x2), int(y2)))\n",
    "\n",
    "# --- Draw bounding boxes on overlay ---\n",
    "overlay_with_boxes = (overlay_ig * 255).astype(np.uint8).copy()\n",
    "overlay_with_boxes = cv2.cvtColor(overlay_with_boxes, cv2.COLOR_RGB2BGR)  # OpenCV expects BGR\n",
    "\n",
    "for (x1, y1, x2, y2) in car_boxes:\n",
    "    cv2.rectangle(overlay_with_boxes, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)  # green boxes\n",
    "\n",
    "overlay_with_boxes = cv2.cvtColor(overlay_with_boxes, cv2.COLOR_BGR2RGB)  # back to RGB for matplotlib\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "# Original\n",
    "plt.subplot(1,4,1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Integrated Gradients Heatmap\n",
    "plt.subplot(1,4,2)\n",
    "plt.title(\"IG Heatmap\")\n",
    "plt.imshow(attr, cmap=\"jet\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Overlay\n",
    "plt.subplot(1,4,3)\n",
    "plt.title(\"Overlay (no boxes)\")\n",
    "plt.imshow(overlay_ig)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Overlay + Bounding Boxes\n",
    "plt.subplot(1,4,4)\n",
    "plt.title(\"Overlay + Car Bounding Boxes\")\n",
    "plt.imshow(overlay_with_boxes)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b286a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# BEST IG TILL NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc7a29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T20:05:33.883336Z",
     "iopub.status.busy": "2025-04-28T20:05:33.882704Z",
     "iopub.status.idle": "2025-04-28T20:05:43.807462Z",
     "shell.execute_reply": "2025-04-28T20:05:43.806765Z",
     "shell.execute_reply.started": "2025-04-28T20:05:33.883311Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from captum.attr import IntegratedGradients\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# --- Speed up ---\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- Load Model ---\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# --- Load and preprocess image ---\n",
    "image_path = \"/kaggle/input/ig-test-on-1st-car-in-oooo7/save as IG test.png\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "resize_size = (640,640)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(resize_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(original_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad_()\n",
    "\n",
    "# --- Blurred baseline ---\n",
    "def blur_baseline(image_tensor, kernel_size=51):\n",
    "    blurred = TF.gaussian_blur(image_tensor.cpu().squeeze(0), kernel_size=kernel_size)\n",
    "    blurred = blurred.unsqueeze(0).to(image_tensor.device)\n",
    "    return blurred\n",
    "\n",
    "baseline = blur_baseline(input_tensor, kernel_size=51)\n",
    "\n",
    "# --- Class Names ---\n",
    "class_names = model.names\n",
    "car_class_index = [k for k, v in class_names.items() if v.lower() == \"car\"]\n",
    "if not car_class_index:\n",
    "    raise ValueError(\"Car class not found.\")\n",
    "car_class_index = car_class_index[0]\n",
    "\n",
    "# --- Define prediction function ---\n",
    "def predict(inputs):\n",
    "    outputs = model.model(inputs)[0]  # shape: (1, N, 5+num_classes)\n",
    "    outputs = outputs[0]\n",
    "    obj_conf = outputs[:, 4]\n",
    "    class_probs = outputs[:, 5:]\n",
    "    car_scores = obj_conf * class_probs[:, car_class_index]\n",
    "    top_idx = torch.argmax(car_scores)\n",
    "    score = car_scores[top_idx]\n",
    "    return score.unsqueeze(0)\n",
    "\n",
    "# --- Integrated Gradients ---\n",
    "ig = IntegratedGradients(predict)\n",
    "\n",
    "# --- Compute attributions ---\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs=input_tensor,\n",
    "    baselines=baseline,\n",
    "    target=None,\n",
    "    return_convergence_delta=True,\n",
    "    n_steps=300,               # more steps = better quality\n",
    "    internal_batch_size=6\n",
    ")\n",
    "\n",
    "# --- Process attributions ---\n",
    "attr = attributions.squeeze().detach().cpu().numpy()\n",
    "attr = np.sum(np.abs(attr), axis=0)  # sum over channels\n",
    "attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-6)  # normalize\n",
    "\n",
    "# --- Prepare image for visualization ---\n",
    "original_np = np.array(original_image.resize(resize_size)) / 255.0\n",
    "\n",
    "# --- Draw bounding boxes ---\n",
    "results = model.predict(original_image, imgsz=resize_size, conf=0.3)\n",
    "boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    cv2.rectangle(original_np, (x1, y1), (x2, y2), color=(1,0,0), thickness=2)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# Plot Original Image with Bounding Boxes\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Original + Detections\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot Integrated Gradients Attribution Map\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Integrated Gradients\")\n",
    "plt.imshow(attr, cmap=\"jet\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot Overlay (heatmap + original)\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Overlay\")\n",
    "heatmap_ig = cv2.applyColorMap(np.uint8(255 * attr), cv2.COLORMAP_JET)\n",
    "heatmap_ig = np.float32(heatmap_ig) / 255\n",
    "overlay_ig = heatmap_ig + original_np\n",
    "overlay_ig = overlay_ig / np.max(overlay_ig)\n",
    "plt.imshow(overlay_ig)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7d045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T20:06:49.252537Z",
     "iopub.status.busy": "2025-04-28T20:06:49.252268Z",
     "iopub.status.idle": "2025-04-28T20:06:49.842858Z",
     "shell.execute_reply": "2025-04-28T20:06:49.842042Z",
     "shell.execute_reply.started": "2025-04-28T20:06:49.252517Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Visualization with multiple colormap options ---\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# Plot Original Image with Bounding Boxes\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Original + Detections\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot Integrated Gradients Attribution Map\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Integrated Gradients\")\n",
    "\n",
    "# Here we use the viridis colormap\n",
    "plt.imshow(attr, cmap='inferno')  # Change 'viridis' to other colormaps like 'plasma', 'inferno', etc.\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot Overlay (heatmap blended with original)\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Overlay\")\n",
    "\n",
    "# Make sure attr is clipped to 0-1\n",
    "attr = np.clip(attr, 0, 1)\n",
    "\n",
    "# Resize heatmap to match the image size (safety)\n",
    "heatmap_attr = cv2.resize(attr, resize_size)\n",
    "\n",
    "# Use colormap, e.g., Viridis (or try 'inferno', 'plasma', etc.)\n",
    "heatmap_ig = cv2.applyColorMap(np.uint8(255 * heatmap_attr), cv2.COLORMAP_VIRIDIS)  # Change here for 'viridis'\n",
    "heatmap_ig = cv2.cvtColor(heatmap_ig, cv2.COLOR_BGR2RGB)  # OpenCV uses BGR, matplotlib expects RGB\n",
    "heatmap_ig = np.float32(heatmap_ig) / 255\n",
    "\n",
    "# Blend heatmap and original image with adjustable transparency (alpha)\n",
    "alpha = 0.5  # You can change alpha here to experiment with transparency\n",
    "overlay_ig = (1 - alpha) * original_np + alpha * heatmap_ig\n",
    "overlay_ig = np.clip(overlay_ig, 0, 1)\n",
    "\n",
    "# Show the final overlay\n",
    "plt.imshow(overlay_ig)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Final display\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564e770",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dfdc3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T18:55:10.124490Z",
     "iopub.status.busy": "2025-04-28T18:55:10.124222Z",
     "iopub.status.idle": "2025-04-28T18:55:10.128097Z",
     "shell.execute_reply": "2025-04-28T18:55:10.127520Z",
     "shell.execute_reply.started": "2025-04-28T18:55:10.124473Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90ee67",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Load Model ---\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a08a29",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(input_tensor.device)\n",
    "print(baseline.device)\n",
    "print(next(model.model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce01517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T07:35:38.742818Z",
     "iopub.status.busy": "2025-04-27T07:35:38.742223Z",
     "iopub.status.idle": "2025-04-27T07:35:38.773439Z",
     "shell.execute_reply": "2025-04-27T07:35:38.772886Z",
     "shell.execute_reply.started": "2025-04-27T07:35:38.742795Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the format of the detections\n",
    "outputs = model(input_tensor)[0]  # Outputs from the YOLO model\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a90694",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## IG on or Inside teh bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a13382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T08:02:04.580544Z",
     "iopub.status.busy": "2025-04-27T08:02:04.579902Z",
     "iopub.status.idle": "2025-04-27T08:02:04.739528Z",
     "shell.execute_reply": "2025-04-27T08:02:04.738774Z",
     "shell.execute_reply.started": "2025-04-27T08:02:04.580524Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load YOLO model (replace with your actual path)\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Load Image (replace with your actual image path)\n",
    "image_path = \"/kaggle/input/kitti-dataset/data_object_image_2/testing/image_2/000007.png\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Preprocess image to match YOLO input\n",
    "resize_size = (384, 1248)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(resize_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(original_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fae49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T08:02:07.978827Z",
     "iopub.status.busy": "2025-04-27T08:02:07.978277Z",
     "iopub.status.idle": "2025-04-27T08:02:08.273537Z",
     "shell.execute_reply": "2025-04-27T08:02:08.272836Z",
     "shell.execute_reply.started": "2025-04-27T08:02:07.978801Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Object Detection on the image\n",
    "outputs = model(input_tensor)[0]  # Model output\n",
    "\n",
    "# The 'outputs' is a 'Results' object. We'll extract the detections\n",
    "detections = outputs.boxes.xyxy.cpu().numpy()  # Bounding boxes (x1, y1, x2, y2)\n",
    "confidences = outputs.boxes.conf.cpu().numpy()  # Confidence scores for the boxes\n",
    "class_probs = outputs.boxes.cls.cpu().numpy()  # Class probabilities for the boxes\n",
    "\n",
    "# Class names (this is a dictionary)\n",
    "class_names = model.names\n",
    "\n",
    "# Print all class names for debugging\n",
    "print(\"Class Names:\", class_names)\n",
    "\n",
    "# Get index of \"Car\" class (case-insensitive search)\n",
    "car_class_index = next((k for k, v in class_names.items() if v.lower() == \"car\"), None)\n",
    "\n",
    "# Check if we found the \"Car\" class and print the index\n",
    "if car_class_index is not None:\n",
    "    print(\"Found Car class at index:\", car_class_index)\n",
    "else:\n",
    "    print(\"Car class not found. Check class names:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c288b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T08:03:03.247028Z",
     "iopub.status.busy": "2025-04-27T08:03:03.246668Z",
     "iopub.status.idle": "2025-04-27T08:03:03.252289Z",
     "shell.execute_reply": "2025-04-27T08:03:03.251531Z",
     "shell.execute_reply.started": "2025-04-27T08:03:03.247007Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Extract Car Detections (Filter with Confidence Threshold) ---\n",
    "car_detections = []\n",
    "\n",
    "# Loop through all detections\n",
    "for i, cls_prob in enumerate(class_probs):\n",
    "    if cls_prob == car_class_index:  # Check if it's a 'car'\n",
    "        confidence = confidences[i]  # Confidence score for the detection\n",
    "        if confidence > 0.5:  # Filter out low confidence detections\n",
    "            car_detections.append(detections[i])  # Add detection to car_detections\n",
    "\n",
    "# Check how many detections we have for 'car'\n",
    "print(f\"Found {len(car_detections)} car detections above 0.5 confidence.\")\n",
    "\n",
    "# Let's view one of the detections for debugging purposes\n",
    "print(\"One of the detected bounding boxes:\", car_detections[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2540c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Image Croppings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db873b4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- DELETE old cropped_outputs folder if it exists ---\n",
    "save_cropped_dir = \"cropped_outputs\"\n",
    "\n",
    "if os.path.exists(save_cropped_dir):\n",
    "    shutil.rmtree(save_cropped_dir)\n",
    "os.makedirs(save_cropped_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62bb41f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64693478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T21:24:43.870493Z",
     "iopub.status.busy": "2025-04-28T21:24:43.870199Z",
     "iopub.status.idle": "2025-04-28T21:24:44.319927Z",
     "shell.execute_reply": "2025-04-28T21:24:44.319286Z",
     "shell.execute_reply.started": "2025-04-28T21:24:43.870475Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Paths ---\n",
    "image_path = \"/kaggle/input/kitti-dataset/data_object_image_2/testing/image_2/000010.png\"   # your input image\n",
    "save_crop_folder = \"cropped_objects\"\n",
    "os.makedirs(save_crop_folder, exist_ok=True)\n",
    "\n",
    "# --- Load Model ---\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# --- Load image ---\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "img_np = np.array(img)\n",
    "\n",
    "# --- Run Detection ---\n",
    "results = model.predict(img, imgsz=(640, 640), conf=0.3)\n",
    "boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "pred_classes = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "class_names = model.names\n",
    "\n",
    "# --- Crop and Save Each Object ---\n",
    "for idx, (box, cls_id) in enumerate(zip(boxes, pred_classes)):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    cropped_obj = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "    class_name = class_names[int(cls_id)]\n",
    "    save_path = os.path.join(save_crop_folder, f\"{idx}_{class_name}.png\")\n",
    "    cropped_obj.save(save_path)\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "print(f\"âœ… Done! Cropped {len(boxes)} objects and saved them in '{save_crop_folder}/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20dea0f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## IG on cropped detection images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97304c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### here th IG is currentluy being calculated on the complete image tensor and not the cropped counded boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bebfc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T21:17:00.392685Z",
     "iopub.status.busy": "2025-04-28T21:17:00.392407Z",
     "iopub.status.idle": "2025-04-28T21:17:00.894452Z",
     "shell.execute_reply": "2025-04-28T21:17:00.893426Z",
     "shell.execute_reply.started": "2025-04-28T21:17:00.392662Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# Load YOLO model\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Load Image\n",
    "image_path = \"/kaggle/working/misclassified/000009.png\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Preprocess image to match YOLO input\n",
    "resize_size = (384, 1248)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(resize_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(original_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad_()\n",
    "\n",
    "# Run Object Detection on the\n",
    "outputs = model(input_tensor)[0]  # Model output\n",
    "\n",
    "# The 'outputs' is a 'Results' object. We'll extract the detections\n",
    "detections = outputs.boxes.xyxy.cpu().numpy()  # Bounding boxes (x1, y1, x2, y2)\n",
    "confidences = outputs.boxes.conf.cpu().numpy()  # Confidence scores for the boxes\n",
    "class_probs = outputs.boxes.cls.cpu().numpy()  # Class probabilities for the boxes\n",
    "\n",
    "# Class names (this is a dictionary)\n",
    "class_names = model.names\n",
    "\n",
    "# Get index of \"Car\" class\n",
    "car_class_index = next((k for k, v in class_names.items() if v.lower() == \"Pedestrian\"), None)\n",
    "\n",
    "# --- Extract Car Detections (Filter with Confidence Threshold) ---\n",
    "car_detections = []\n",
    "\n",
    "# Loop through all detections\n",
    "for i, cls_prob in enumerate(class_probs):\n",
    "    if cls_prob == car_class_index:  # Check if it's a 'car'\n",
    "        confidence = confidences[i]  # Confidence score for the detection\n",
    "        if confidence > 0.5:  # Filter out low confidence detections\n",
    "            car_detections.append(detections[i])  # Add detection to car_detections\n",
    "\n",
    "# Print how many detections we have for 'car'\n",
    "print(f\"Found {len(car_detections)} car detections above 0.5 confidence.\")\n",
    "\n",
    "# Let's view one of the detections for debugging purposes\n",
    "print(\"One of the detected bounding boxes:\", car_detections[0])\n",
    "\n",
    "# Define your Integrated Gradients object\n",
    "ig = IntegratedGradients(predict)\n",
    "\n",
    "# Visualize attributions inside the bounding boxes\n",
    "for det in car_detections:\n",
    "    x1, y1, x2, y2 = map(int, det[:4])  # Bounding box coordinates\n",
    "    bbox_image = np.array(original_image)[y1:y2, x1:x2, :]  # Crop the image inside the bounding box\n",
    "\n",
    "    # Convert to tensor and preprocess for IG\n",
    "    bbox_tensor = preprocess(Image.fromarray(bbox_image)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Create a baseline (black image) for IG\n",
    "    baseline = torch.zeros_like(bbox_tensor).to(device)\n",
    "\n",
    "    # Compute IG for the Bounding Box Region\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs=bbox_tensor,\n",
    "        baselines=baseline,\n",
    "        target=None,\n",
    "        return_convergence_delta=True,\n",
    "        n_steps=50,  # More steps for better resolution\n",
    "        internal_batch_size=4  # Suitable batch size\n",
    "    )\n",
    "\n",
    "    # Process the attributions to visualize them\n",
    "    attr = attributions.squeeze().detach().cpu().numpy()\n",
    "    attr = np.sum(np.abs(attr), axis=0)  # Sum over the channels (RGB)\n",
    "    attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-6)  # Normalize the attribution map\n",
    "\n",
    "    # --- Prepare for Visualization ---\n",
    "    # Apply a colormap to the attribution map\n",
    "    heatmap_ig = cv2.applyColorMap(np.uint8(255 * attr), cv2.COLORMAP_VIRIDIS)\n",
    "    heatmap_ig = np.float32(heatmap_ig) / 255  # Normalize to 0-1 range\n",
    "\n",
    "    # Resize the heatmap to match the size of the bounding box\n",
    "    heatmap_ig_resized = cv2.resize(heatmap_ig, (bbox_image.shape[1], bbox_image.shape[0]))\n",
    "\n",
    "    # Overlay the resized heatmap on the original cropped image\n",
    "    overlay_ig = heatmap_ig_resized + bbox_image / 255.0\n",
    "    overlay_ig = overlay_ig / np.max(overlay_ig)  # Normalize to [0, 1]\n",
    "\n",
    "    # --- Plot Results ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Original ROI (Bounding Box)\")\n",
    "    plt.imshow(bbox_image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Attribution Map\")\n",
    "    plt.imshow(attr, cmap=\"viridis\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Overlay\")\n",
    "    plt.imshow(overlay_ig)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59367114",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8972a2e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111b487",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f71ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T19:52:13.640426Z",
     "iopub.status.busy": "2025-04-28T19:52:13.639952Z",
     "iopub.status.idle": "2025-04-28T19:52:23.549244Z",
     "shell.execute_reply": "2025-04-28T19:52:23.548475Z",
     "shell.execute_reply.started": "2025-04-28T19:52:13.640403Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from captum.attr import IntegratedGradients\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# --- Speed up ---\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- Load Model ---\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# --- Resize with padding function (Letterbox style) ---\n",
    "def resize_with_padding(image, target_size=(640, 640)):\n",
    "    old_w, old_h = image.size\n",
    "    target_w, target_h = target_size\n",
    "    scale = min(target_w / old_w, target_h / old_h)\n",
    "    new_w, new_h = int(old_w * scale), int(old_h * scale)\n",
    "    resized_image = image.resize((new_w, new_h), Image.BILINEAR)\n",
    "    new_image = Image.new(\"RGB\", target_size, (0, 0, 0))  # black padding\n",
    "    paste_x = (target_w - new_w) // 2\n",
    "    paste_y = (target_h - new_h) // 2\n",
    "    new_image.paste(resized_image, (paste_x, paste_y))\n",
    "    return new_image, scale, paste_x, paste_y\n",
    "\n",
    "# --- Blurred baseline ---\n",
    "def blur_baseline(image_tensor, kernel_size=51):\n",
    "    blurred = TF.gaussian_blur(image_tensor.cpu().squeeze(0), kernel_size=kernel_size)\n",
    "    blurred = blurred.unsqueeze(0).to(image_tensor.device)\n",
    "    return blurred\n",
    "\n",
    "# --- Load and preprocess image ---\n",
    "image_path = \"/kaggle/input/car-imge/car_lifestyle-02.jpg\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "resize_size = (640, 640)\n",
    "resized_padded_image, scale, pad_x, pad_y = resize_with_padding(original_image, target_size=resize_size)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(resized_padded_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad_()\n",
    "\n",
    "baseline = blur_baseline(input_tensor, kernel_size=51)\n",
    "\n",
    "# --- Class Names ---\n",
    "class_names = model.names\n",
    "car_class_index = [k for k, v in class_names.items() if v.lower() == \"car\"]\n",
    "if not car_class_index:\n",
    "    raise ValueError(\"Car class not found.\")\n",
    "car_class_index = car_class_index[0]\n",
    "\n",
    "# --- Define prediction function ---\n",
    "def predict(inputs):\n",
    "    outputs = model.model(inputs)[0]  # shape: (1, N, 5+num_classes)\n",
    "    outputs = outputs[0]\n",
    "    obj_conf = outputs[:, 4]\n",
    "    class_probs = outputs[:, 5:]\n",
    "    car_scores = obj_conf * class_probs[:, car_class_index]\n",
    "    top_idx = torch.argmax(car_scores)\n",
    "    score = car_scores[top_idx]\n",
    "    return score.unsqueeze(0)\n",
    "\n",
    "# --- Integrated Gradients ---\n",
    "ig = IntegratedGradients(predict)\n",
    "\n",
    "# --- Compute attributions ---\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs=input_tensor,\n",
    "    baselines=baseline,\n",
    "    target=None,\n",
    "    return_convergence_delta=True,\n",
    "    n_steps=300,\n",
    "    internal_batch_size=6\n",
    ")\n",
    "\n",
    "# --- Process attributions ---\n",
    "attr = attributions.squeeze().detach().cpu().numpy()\n",
    "attr = np.sum(np.abs(attr), axis=0)  # sum over channels\n",
    "attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-6)  # normalize\n",
    "\n",
    "# --- Run Detection ---\n",
    "results = model.predict(resized_padded_image, imgsz=resize_size, conf=0.3)\n",
    "boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "\n",
    "# --- Prepare image for visualization ---\n",
    "original_np = np.array(resized_padded_image) / 255.0\n",
    "\n",
    "# --- Draw bounding boxes ---\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    cv2.rectangle(original_np, (x1, y1), (x2, y2), color=(1,0,0), thickness=2)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(18,6))\n",
    "\n",
    "# Original + Detections\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Original + Detections\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Integrated Gradients Heatmap\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Integrated Gradients (viridis)\")\n",
    "plt.imshow(attr, cmap=\"viridis\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Overlay (Heatmap + Original)\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Overlay\")\n",
    "heatmap_ig = cv2.applyColorMap(np.uint8(255 * attr), cv2.COLORMAP_VIRIDIS)\n",
    "heatmap_ig = np.float32(heatmap_ig) / 255\n",
    "overlay_ig = heatmap_ig + original_np\n",
    "overlay_ig = overlay_ig / np.max(overlay_ig)\n",
    "plt.imshow(overlay_ig)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553409bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T20:13:04.692854Z",
     "iopub.status.busy": "2025-04-28T20:13:04.692093Z",
     "iopub.status.idle": "2025-04-28T20:13:04.696023Z",
     "shell.execute_reply": "2025-04-28T20:13:04.695445Z",
     "shell.execute_reply.started": "2025-04-28T20:13:04.692822Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df779c24",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# BEST IG + work on BOUNDING BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f389f04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T21:29:44.037416Z",
     "iopub.status.busy": "2025-04-28T21:29:44.036813Z",
     "iopub.status.idle": "2025-04-28T21:29:55.254942Z",
     "shell.execute_reply": "2025-04-28T21:29:55.254203Z",
     "shell.execute_reply.started": "2025-04-28T21:29:44.037393Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from captum.attr import IntegratedGradients\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# --- Speed up ---\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- Load Model ---\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# --- Resize with padding function ---\n",
    "def resize_with_padding(image, target_size=(640, 640)):\n",
    "    old_w, old_h = image.size\n",
    "    target_w, target_h = target_size\n",
    "    scale = min(target_w / old_w, target_h / old_h)\n",
    "    new_w, new_h = int(old_w * scale), int(old_h * scale)\n",
    "    resized_image = image.resize((new_w, new_h), Image.BILINEAR)\n",
    "    new_image = Image.new(\"RGB\", target_size, (0, 0, 0))  # black padding\n",
    "    paste_x = (target_w - new_w) // 2\n",
    "    paste_y = (target_h - new_h) // 2\n",
    "    new_image.paste(resized_image, (paste_x, paste_y))\n",
    "    return new_image, scale, paste_x, paste_y\n",
    "\n",
    "# --- Blurred baseline ---\n",
    "def blur_baseline(image_tensor, kernel_size=51):\n",
    "    blurred = TF.gaussian_blur(image_tensor.cpu().squeeze(0), kernel_size=kernel_size)\n",
    "    blurred = blurred.unsqueeze(0).to(image_tensor.device)\n",
    "    return blurred\n",
    "\n",
    "# --- Load and preprocess image ---\n",
    "# image_path = \"/kaggle/input/ig-test-on-1st-car-in-oooo7/save as IG test.png\"\n",
    "image_path =\"/kaggle/working/cropped_objects/5_Van.png\"\n",
    "# image_path = \"/kaggle/working/misclassified/missed_car_000027.png\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# --- Change these target sizes here ---\n",
    "# target_size = (736, 320)    # <- first try\n",
    "target_size = (1248,384)    # <- then change to this\n",
    "\n",
    "resized_padded_image, scale, pad_x, pad_y = resize_with_padding(original_image, target_size=target_size)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(resized_padded_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad_()\n",
    "\n",
    "baseline = blur_baseline(input_tensor, kernel_size=51)\n",
    "\n",
    "# --- Class Names ---\n",
    "class_names = model.names\n",
    "car_class_index = [k for k, v in class_names.items() if v.lower() == \"car\"]\n",
    "if not car_class_index:\n",
    "    raise ValueError(\"Car class not found.\")\n",
    "car_class_index = car_class_index[0]\n",
    "\n",
    "# --- Define prediction function ---\n",
    "def predict(inputs):\n",
    "    outputs = model.model(inputs)[0]  # shape: (1, N, 5+num_classes)\n",
    "    outputs = outputs[0]\n",
    "    obj_conf = outputs[:, 4]\n",
    "    class_probs = outputs[:, 5:]\n",
    "    car_scores = obj_conf * class_probs[:, car_class_index]\n",
    "    top_idx = torch.argmax(car_scores)\n",
    "    score = car_scores[top_idx]\n",
    "    return score.unsqueeze(0)\n",
    "\n",
    "# --- Integrated Gradients ---\n",
    "ig = IntegratedGradients(predict)\n",
    "\n",
    "# --- Compute attributions ---\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs=input_tensor,\n",
    "    baselines=baseline,\n",
    "    target=None,\n",
    "    return_convergence_delta=True,\n",
    "    n_steps=300,\n",
    "    internal_batch_size=6\n",
    ")\n",
    "\n",
    "# --- Process attributions ---\n",
    "attr = attributions.squeeze().detach().cpu().numpy()\n",
    "attr = np.sum(np.abs(attr), axis=0)\n",
    "attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-6)\n",
    "\n",
    "# --- Run Detection ---\n",
    "results = model.predict(resized_padded_image, imgsz=target_size, conf=0.3)\n",
    "boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "\n",
    "# --- Prepare image for visualization ---\n",
    "original_np = np.array(resized_padded_image) / 255.0\n",
    "\n",
    "# --- Draw bounding boxes ---\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    cv2.rectangle(original_np, (x1, y1), (x2, y2), color=(1,0,0), thickness=2)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(18,6))\n",
    "\n",
    "# Original + Detections\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(f\"Original + Detections ({target_size[0]}x{target_size[1]})\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Integrated Gradients Heatmap\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Integrated Gradients (viridis)\")\n",
    "plt.imshow(attr, cmap=\"viridis\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Overlay (Heatmap + Original)\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Overlay\")\n",
    "heatmap_ig = cv2.applyColorMap(np.uint8(255 * attr), cv2.COLORMAP_VIRIDIS)\n",
    "heatmap_ig = np.float32(heatmap_ig) / 255\n",
    "overlay_ig = heatmap_ig + original_np\n",
    "overlay_ig = overlay_ig / np.max(overlay_ig)\n",
    "plt.imshow(overlay_ig)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed7489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T21:30:25.584656Z",
     "iopub.status.busy": "2025-04-28T21:30:25.584378Z",
     "iopub.status.idle": "2025-04-28T21:30:26.265512Z",
     "shell.execute_reply": "2025-04-28T21:30:26.264831Z",
     "shell.execute_reply.started": "2025-04-28T21:30:25.584635Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,18))  # (width, height) -> make it taller now\n",
    "\n",
    "# 1st Plot: Original + Detections\n",
    "plt.subplot(3,1,1)\n",
    "plt.title(f\"Original + Detections ({target_size[0]}x{target_size[1]})\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# 2nd Plot: Integrated Gradients\n",
    "plt.subplot(3,1,2)\n",
    "plt.title(\"Integrated Gradients (viridis)\")\n",
    "plt.imshow(attr, cmap=\"viridis\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# 3rd Plot: Overlay\n",
    "plt.subplot(3,1,3)\n",
    "plt.title(\"Overlay\")\n",
    "heatmap_ig = cv2.applyColorMap(np.uint8(255 * attr), cv2.COLORMAP_VIRIDIS)\n",
    "heatmap_ig = np.float32(heatmap_ig) / 255\n",
    "overlay_ig = heatmap_ig + original_np\n",
    "overlay_ig = overlay_ig / np.max(overlay_ig)\n",
    "plt.imshow(overlay_ig)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81805694",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Saving the above work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9720c18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T20:15:49.857627Z",
     "iopub.status.busy": "2025-04-28T20:15:49.856781Z",
     "iopub.status.idle": "2025-04-28T20:15:51.733826Z",
     "shell.execute_reply": "2025-04-28T20:15:51.733114Z",
     "shell.execute_reply.started": "2025-04-28T20:15:49.857600Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Visualization and Saving ---\n",
    "save_dir = \"saved_outputs\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create folder if not exists\n",
    "\n",
    "plt.figure(figsize=(8, 24))  # Taller figure for vertical layout\n",
    "\n",
    "# Original + Detections\n",
    "plt.subplot(3,1,1)\n",
    "plt.title(f\"Original + Detections ({target_size[0]}x{target_size[1]})\")\n",
    "plt.imshow(original_np)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(os.path.join(save_dir, \"Original_with_Detections.png\"), bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "# Integrated Gradients Heatmap\n",
    "plt.subplot(3,1,2)\n",
    "plt.title(\"Integrated Gradients (viridis)\")\n",
    "plt.imshow(attr, cmap=\"viridis\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(os.path.join(save_dir, \"IG_Heatmap.png\"), bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "# Overlay (Heatmap + Original)\n",
    "plt.subplot(3,1,3)\n",
    "plt.title(\"Overlay\")\n",
    "heatmap_ig = cv2.applyColorMap(np.uint8(255 * attr), cv2.COLORMAP_VIRIDIS)\n",
    "heatmap_ig = np.float32(heatmap_ig) / 255\n",
    "overlay_ig = heatmap_ig + original_np\n",
    "overlay_ig = overlay_ig / np.max(overlay_ig)\n",
    "plt.imshow(overlay_ig)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(os.path.join(save_dir, \"Overlay.png\"), bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved all images to '{save_dir}/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac863ac3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7899166",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3fb14a7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# getting the misclassfied images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e057d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T20:49:20.986367Z",
     "iopub.status.busy": "2025-04-28T20:49:20.985716Z",
     "iopub.status.idle": "2025-04-28T20:49:20.991667Z",
     "shell.execute_reply": "2025-04-28T20:49:20.991029Z",
     "shell.execute_reply.started": "2025-04-28T20:49:20.986342Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- DELETE old misclassified folder if it exists ---\n",
    "if os.path.exists(save_misclassified_dir):\n",
    "    shutil.rmtree(save_misclassified_dir)\n",
    "os.makedirs(save_misclassified_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6514e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T20:54:11.747491Z",
     "iopub.status.busy": "2025-04-28T20:54:11.746746Z",
     "iopub.status.idle": "2025-04-28T20:54:14.131883Z",
     "shell.execute_reply": "2025-04-28T20:54:14.131346Z",
     "shell.execute_reply.started": "2025-04-28T20:54:11.747464Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Paths ---\n",
    "image_folder = \"/kaggle/input/kitti-dataset/data_object_image_2/testing/image_2/\"\n",
    "save_misclassified_dir = \"misclassified\"\n",
    "\n",
    "# --- DELETE old misclassified folder if it exists ---\n",
    "if os.path.exists(save_misclassified_dir):\n",
    "    shutil.rmtree(save_misclassified_dir)\n",
    "os.makedirs(save_misclassified_dir, exist_ok=True)\n",
    "\n",
    "# --- Load Model ---\n",
    "model_path = \"/kaggle/input/trained_yolo-model/pytorch/default/1/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# --- Class Mapping ---\n",
    "class_names = model.names\n",
    "\n",
    "# --- Thresholds ---\n",
    "CONFIDENCE_THRESHOLD = 0.7   # probability threshold for \"good\" detection\n",
    "\n",
    "# --- Main Logic ---\n",
    "misclassified_count = 0\n",
    "max_to_find = 5\n",
    "\n",
    "image_paths = sorted(glob(os.path.join(image_folder, \"*.png\")))\n",
    "\n",
    "for img_path in image_paths:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    results = model.predict(img, imgsz=(640, 640), conf=0.1)  # Keep low conf initially\n",
    "    preds = results[0].boxes\n",
    "\n",
    "    suspicious = False\n",
    "\n",
    "    if preds is None or len(preds) == 0:\n",
    "        # No detections at all\n",
    "        suspicious = True\n",
    "    else:\n",
    "        pred_classes = preds.cls.cpu().numpy()\n",
    "        pred_scores = preds.conf.cpu().numpy()\n",
    "        pred_class_names = [class_names[int(c)].lower() for c in pred_classes]\n",
    "\n",
    "        # Check conditions:\n",
    "        if pred_scores.max() < CONFIDENCE_THRESHOLD:\n",
    "            suspicious = True\n",
    "        elif not any(cls in ['car', 'truck', 'van'] for cls in pred_class_names):\n",
    "            suspicious = True\n",
    "\n",
    "    if suspicious:\n",
    "        filename = os.path.basename(img_path)\n",
    "        save_path = os.path.join(save_misclassified_dir, filename)\n",
    "        shutil.copy(img_path, save_path)\n",
    "        misclassified_count += 1\n",
    "        print(f\"Saved suspicious image: {save_path}\")\n",
    "\n",
    "    if misclassified_count >= max_to_find:\n",
    "        break\n",
    "\n",
    "print(f\"âœ… Done! Saved {misclassified_count} suspicious/misclassified images in '{save_misclassified_dir}/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e196a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 976194,
     "sourceId": 1650695,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3673013,
     "sourceId": 6374376,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7263516,
     "sourceId": 11584536,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7263545,
     "sourceId": 11584576,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7278510,
     "sourceId": 11604583,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 318996,
     "modelInstanceId": 298391,
     "sourceId": 358174,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 318999,
     "modelInstanceId": 298394,
     "sourceId": 358178,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 86.32664,
   "end_time": "2025-04-28T21:36:15.916845",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-28T21:34:49.590205",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
